{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Figure 4 Analysis: Phenotype Complexity vs Performance\n",
    "\n",
    "This notebook analyzes how ELDER and ontology-only approaches perform across different phenotype complexity levels.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's import the required libraries and set up our analysis environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import duckdb\n",
    "\n",
    "# Set up paths\n",
    "DATA_DIR = Path(\"../data\")\n",
    "RESULTS_DIR = Path(\"../results\")\n",
    "RESULTS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "# Configuration\n",
    "DEFAULT_DATASET = \"normalized_phenopackets_07\"  # Best ELDER performance\n",
    "DB_PATH = DATA_DIR / \"Best_Match_Cosine_all_combined_Elder_vs_Exomiser.db\"\n",
    "\n",
    "print(f\"Data directory: {DATA_DIR}\")\n",
    "print(f\"Results directory: {RESULTS_DIR}\")\n",
    "print(f\"Default dataset: {DEFAULT_DATASET}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Available Datasets\n",
    "\n",
    "Let's see what normalized datasets are available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List available datasets\n",
    "available_datasets = []\n",
    "for item in DATA_DIR.iterdir():\n",
    "    if item.is_dir() and item.name.startswith(\"normalized_phenopackets\"):\n",
    "        file_count = len(list(item.glob(\"*.json\")))\n",
    "        available_datasets.append((item.name, file_count))\n",
    "\n",
    "print(\"Available normalized datasets:\")\n",
    "for dataset, count in sorted(available_datasets):\n",
    "    marker = \" ‚≠ê\" if dataset == DEFAULT_DATASET else \"\"\n",
    "    print(f\"  {dataset}: {count} files{marker}\")\n",
    "\n",
    "print(f\"\\n‚≠ê {DEFAULT_DATASET} is the default (best ELDER performance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Analysis Functions\n",
    "\n",
    "Let's define the core analysis functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset_and_bin_by_complexity(dataset_path):\n",
    "    \"\"\"\n",
    "    Load phenopackets and assign them to complexity bins.\n",
    "    \n",
    "    Returns:\n",
    "        - bins: Dictionary mapping bin names to lists of filenames\n",
    "        - file_to_disease: Dictionary mapping filenames to disease IDs\n",
    "    \"\"\"\n",
    "    bins = {\n",
    "        \"1-5\": [],\n",
    "        \"10-15\": [],\n",
    "        \"15-20\": [],\n",
    "        \"20-25\": [],\n",
    "        \"25-30\": [],\n",
    "        \"30-35\": [],\n",
    "        \"35-40\": [],\n",
    "        \"40-45\": [],\n",
    "        \"45+\": [],\n",
    "    }\n",
    "    \n",
    "    file_to_disease = {}\n",
    "    phenotype_counts = []\n",
    "    \n",
    "    print(f\"Loading dataset from: {dataset_path}\")\n",
    "    \n",
    "    for json_file in dataset_path.glob(\"*.json\"):\n",
    "        try:\n",
    "            with open(json_file, 'r') as f:\n",
    "                phenopacket = json.load(f)\n",
    "            \n",
    "            # Extract disease ID\n",
    "            if 'diseases' in phenopacket and phenopacket['diseases']:\n",
    "                disease_id = phenopacket['diseases'][0]['term']['id']\n",
    "                file_to_disease[json_file.name] = disease_id\n",
    "            \n",
    "            # Count phenotypes\n",
    "            phenotype_count = 0\n",
    "            if 'phenotypicFeatures' in phenopacket:\n",
    "                phenotype_count = len(phenopacket['phenotypicFeatures'])\n",
    "            \n",
    "            phenotype_counts.append(phenotype_count)\n",
    "            \n",
    "            # Assign to bins\n",
    "            filename = json_file.name\n",
    "            if 1 <= phenotype_count <= 5:\n",
    "                bins[\"1-5\"].append(filename)\n",
    "            elif 10 <= phenotype_count <= 15:\n",
    "                bins[\"10-15\"].append(filename)\n",
    "            elif 16 <= phenotype_count <= 20:\n",
    "                bins[\"15-20\"].append(filename)\n",
    "            elif 21 <= phenotype_count <= 25:\n",
    "                bins[\"20-25\"].append(filename)\n",
    "            elif 26 <= phenotype_count <= 30:\n",
    "                bins[\"25-30\"].append(filename)\n",
    "            elif 31 <= phenotype_count <= 35:\n",
    "                bins[\"30-35\"].append(filename)\n",
    "            elif 36 <= phenotype_count <= 40:\n",
    "                bins[\"35-40\"].append(filename)\n",
    "            elif 41 <= phenotype_count <= 45:\n",
    "                bins[\"40-45\"].append(filename)\n",
    "            elif phenotype_count > 45:\n",
    "                bins[\"45+\"].append(filename)\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {json_file}: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # Print bin summary\n",
    "    total_binned = sum(len(files) for files in bins.values())\n",
    "    print(f\"\\nDataset summary:\")\n",
    "    print(f\"  Total files processed: {len(phenotype_counts)}\")\n",
    "    print(f\"  Files assigned to bins: {total_binned}\")\n",
    "    print(f\"  Average phenotypes per case: {np.mean(phenotype_counts):.1f}\")\n",
    "    \n",
    "    print(f\"\\nBin distribution:\")\n",
    "    for bin_name, files in bins.items():\n",
    "        print(f\"  {bin_name}: {len(files)} files\")\n",
    "    \n",
    "    return bins, file_to_disease"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_performance_data(bins, db_path):\n",
    "    \"\"\"\n",
    "    Query the database for ELDER and Exomiser performance data.\n",
    "    \n",
    "    Returns:\n",
    "        - elder_per_case: Dictionary mapping filenames to ELDER top-1 accuracy\n",
    "        - exomiser_per_case: Dictionary mapping filenames to Exomiser top-1 accuracy\n",
    "    \"\"\"\n",
    "    elder_per_case = {}\n",
    "    exomiser_per_case = {}\n",
    "    \n",
    "    print(f\"\\nQuerying database: {db_path}\")\n",
    "    \n",
    "    if not db_path.exists():\n",
    "        print(f\"‚ùå Error: Database not found at {db_path}\")\n",
    "        print(\"Please download the database from the Zenodo link provided in README.md\")\n",
    "        return elder_per_case, exomiser_per_case\n",
    "    \n",
    "    conn = duckdb.connect(str(db_path))\n",
    "    \n",
    "    # Get all filenames from all bins\n",
    "    all_files = []\n",
    "    for bin_files in bins.values():\n",
    "        all_files.extend(bin_files)\n",
    "    \n",
    "    print(f\"Querying performance data for {len(all_files)} files...\")\n",
    "    \n",
    "    successful_queries = 0\n",
    "    \n",
    "    for filename in all_files:\n",
    "        query = \"\"\"\n",
    "        SELECT Exomiser, cosBMA_ELDER_large3\n",
    "        FROM Exomiser_vs_cosBMA_ELDER_large3_disease_rank_comparison\n",
    "        WHERE phenopacket = ?\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            result = conn.execute(query, [filename]).fetchone()\n",
    "            if result:\n",
    "                exomiser_rank, elder_rank = result\n",
    "                # Convert rank to top-1 accuracy (1 if rank=1, 0 otherwise)\n",
    "                exomiser_top1 = 1 if exomiser_rank == 1 else 0\n",
    "                elder_top1 = 1 if elder_rank == 1 else 0\n",
    "                \n",
    "                exomiser_per_case[filename] = exomiser_top1\n",
    "                elder_per_case[filename] = elder_top1\n",
    "                successful_queries += 1\n",
    "            else:\n",
    "                exomiser_per_case[filename] = 0\n",
    "                elder_per_case[filename] = 0\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Database query error for {filename}: {e}\")\n",
    "            exomiser_per_case[filename] = 0\n",
    "            elder_per_case[filename] = 0\n",
    "    \n",
    "    conn.close()\n",
    "    \n",
    "    print(f\"‚úÖ Successfully queried {successful_queries}/{len(all_files)} files\")\n",
    "    \n",
    "    return elder_per_case, exomiser_per_case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bin_accuracies(bins, elder_per_case, exomiser_per_case):\n",
    "    \"\"\"\n",
    "    Calculate average top-1 accuracy for each phenotype complexity bin.\n",
    "    \n",
    "    Returns:\n",
    "        - elder_bin_accuracies: Dictionary mapping bin names to ELDER accuracies\n",
    "        - exomiser_bin_accuracies: Dictionary mapping bin names to Exomiser accuracies\n",
    "    \"\"\"\n",
    "    elder_bin_accuracies = {}\n",
    "    exomiser_bin_accuracies = {}\n",
    "    \n",
    "    print(f\"\\nCalculating bin-wise accuracies:\")\n",
    "    \n",
    "    for bin_name, files in bins.items():\n",
    "        if files:\n",
    "            elder_accuracies = [elder_per_case.get(f, 0) for f in files]\n",
    "            exomiser_accuracies = [exomiser_per_case.get(f, 0) for f in files]\n",
    "            \n",
    "            elder_bin_accuracies[bin_name] = sum(elder_accuracies) / len(elder_accuracies)\n",
    "            exomiser_bin_accuracies[bin_name] = sum(exomiser_accuracies) / len(exomiser_accuracies)\n",
    "            \n",
    "            print(f\"  Bin {bin_name}: {len(files)} files\")\n",
    "            print(f\"    ELDER: {elder_bin_accuracies[bin_name]:.3f}\")\n",
    "            print(f\"    Ontology-only: {exomiser_bin_accuracies[bin_name]:.3f}\")\n",
    "            print(f\"    Difference: {elder_bin_accuracies[bin_name] - exomiser_bin_accuracies[bin_name]:+.3f}\")\n",
    "        else:\n",
    "            elder_bin_accuracies[bin_name] = 0\n",
    "            exomiser_bin_accuracies[bin_name] = 0\n",
    "    \n",
    "    return elder_bin_accuracies, exomiser_bin_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_figure(elder_bin_accuracies, exomiser_bin_accuracies, dataset_name, save_plots=True):\n",
    "    \"\"\"\n",
    "    Create the comparative bar chart (Figure 4).\n",
    "    \n",
    "    Parameters:\n",
    "        - elder_bin_accuracies: ELDER accuracies by bin\n",
    "        - exomiser_bin_accuracies: Exomiser accuracies by bin\n",
    "        - dataset_name: Name of dataset being analyzed\n",
    "        - save_plots: Whether to save the plots to files\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    \n",
    "    bin_names = list(elder_bin_accuracies.keys())\n",
    "    elder_accuracies = list(elder_bin_accuracies.values())\n",
    "    exomiser_accuracies = list(exomiser_bin_accuracies.values())\n",
    "    \n",
    "    x = np.arange(len(bin_names))\n",
    "    width = 0.35\n",
    "    \n",
    "    # Create bars\n",
    "    bars1 = plt.bar(x - width/2, exomiser_accuracies, width, label='Ontology-only', \n",
    "                   color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "    bars2 = plt.bar(x + width/2, elder_accuracies, width, label='ELDER', \n",
    "                   color='skyblue', edgecolor='black', alpha=0.7)\n",
    "    \n",
    "    # Formatting\n",
    "    plt.xlabel('Phenotype Count Bins', fontsize=12)\n",
    "    plt.ylabel('Average Top-1 Accuracy', fontsize=12)\n",
    "    plt.ylim(0, 1.0)\n",
    "    plt.xticks(x, bin_names)\n",
    "    plt.legend(fontsize=11)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for bar, accuracy in zip(bars1, exomiser_accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{accuracy:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    for bar, accuracy in zip(bars2, elder_accuracies):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
    "                f'{accuracy:.3f}', ha='center', va='bottom', fontweight='bold', fontsize=9)\n",
    "    \n",
    "    plt.grid(True, alpha=0.3, axis='y')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Add subtitle with dataset info\n",
    "    plt.figtext(0.5, 0.02, f'Dataset: {dataset_name}', ha='center', fontsize=10, style='italic')\n",
    "    \n",
    "    if save_plots:\n",
    "        # Save plots\n",
    "        svg_path = RESULTS_DIR / 'figure_4_phenotype_complexity.svg'\n",
    "        png_path = RESULTS_DIR / 'figure_4_phenotype_complexity.png'\n",
    "        \n",
    "        plt.savefig(svg_path, bbox_inches='tight')\n",
    "        plt.savefig(png_path, dpi=300, bbox_inches='tight')\n",
    "        \n",
    "        print(f\"\\nüíæ Plots saved:\")\n",
    "        print(f\"  SVG: {svg_path}\")\n",
    "        print(f\"  PNG: {png_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate overall performance\n",
    "    elder_overall = np.mean(elder_accuracies)\n",
    "    exomiser_overall = np.mean(exomiser_accuracies)\n",
    "    difference = elder_overall - exomiser_overall\n",
    "    \n",
    "    print(f\"\\nüìä Overall Performance Summary:\")\n",
    "    print(f\"  ELDER average: {elder_overall:.3f}\")\n",
    "    print(f\"  Ontology-only average: {exomiser_overall:.3f}\")\n",
    "    print(f\"  Difference: {difference:+.3f} ({'ELDER better' if difference > 0 else 'Ontology-only better'})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Analysis\n",
    "\n",
    "Now let's run the complete analysis pipeline:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select dataset to analyze\n",
    "DATASET_TO_ANALYZE = DEFAULT_DATASET  # Change this to use a different dataset\n",
    "dataset_path = DATA_DIR / DATASET_TO_ANALYZE\n",
    "\n",
    "print(f\"üî¨ Running Figure 4 Analysis\")\n",
    "print(f\"üìÅ Dataset: {DATASET_TO_ANALYZE}\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Step 1: Load dataset and bin by complexity\n",
    "bins, file_to_disease = load_dataset_and_bin_by_complexity(dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Query performance data from database\n",
    "elder_per_case, exomiser_per_case = query_performance_data(bins, DB_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Calculate bin-wise accuracies\n",
    "elder_bin_accuracies, exomiser_bin_accuracies = calculate_bin_accuracies(\n",
    "    bins, elder_per_case, exomiser_per_case\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Create and display Figure 4\n",
    "create_figure(elder_bin_accuracies, exomiser_bin_accuracies, DATASET_TO_ANALYZE, save_plots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Different Datasets (Optional)\n",
    "\n",
    "You can run this cell to quickly compare how different datasets perform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare multiple datasets (optional)\n",
    "COMPARE_DATASETS = True  # Set to False to skip this comparison\n",
    "\n",
    "if COMPARE_DATASETS and len(available_datasets) > 1:\n",
    "    print(\"üîÑ Comparing multiple datasets...\\n\")\n",
    "    \n",
    "    dataset_comparisons = []\n",
    "    \n",
    "    # Analyze first 3 datasets for comparison\n",
    "    for dataset_name, _ in available_datasets[:3]:\n",
    "        print(f\"Analyzing {dataset_name}...\")\n",
    "        \n",
    "        dataset_path = DATA_DIR / dataset_name\n",
    "        bins_temp, _ = load_dataset_and_bin_by_complexity(dataset_path)\n",
    "        elder_temp, exomiser_temp = query_performance_data(bins_temp, DB_PATH)\n",
    "        elder_acc_temp, exomiser_acc_temp = calculate_bin_accuracies(\n",
    "            bins_temp, elder_temp, exomiser_temp\n",
    "        )\n",
    "        \n",
    "        # Calculate overall performance\n",
    "        elder_overall = np.mean(list(elder_acc_temp.values()))\n",
    "        exomiser_overall = np.mean(list(exomiser_acc_temp.values()))\n",
    "        difference = elder_overall - exomiser_overall\n",
    "        \n",
    "        dataset_comparisons.append({\n",
    "            'dataset': dataset_name,\n",
    "            'elder_accuracy': elder_overall,\n",
    "            'exomiser_accuracy': exomiser_overall,\n",
    "            'difference': difference\n",
    "        })\n",
    "        \n",
    "        print(f\"  ELDER: {elder_overall:.3f}, Ontology-only: {exomiser_overall:.3f}, Diff: {difference:+.3f}\\n\")\n",
    "    \n",
    "    # Sort by difference\n",
    "    dataset_comparisons.sort(key=lambda x: x['difference'], reverse=True)\n",
    "    \n",
    "    print(\"üìä Dataset Comparison Summary:\")\n",
    "    for i, result in enumerate(dataset_comparisons, 1):\n",
    "        marker = \" ‚≠ê\" if result['dataset'] == DEFAULT_DATASET else \"\"\n",
    "        print(f\"{i}. {result['dataset']}: ELDER advantage = {result['difference']:+.3f}{marker}\")\n",
    "else:\n",
    "    print(\"Skipping dataset comparison...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has generated Figure 4, which shows:\n",
    "\n",
    "1. **Performance comparison** between ELDER and ontology-only approaches\n",
    "2. **Phenotype complexity analysis** across 9 different complexity bins\n",
    "3. **Top-1 accuracy metrics** for both approaches\n",
    "\n",
    "### Key Outputs:\n",
    "- `figure_4_phenotype_complexity.svg` - Vector format for publication\n",
    "- `figure_4_phenotype_complexity.png` - Raster format for presentations\n",
    "- Console output with detailed accuracy statistics\n",
    "\n",
    "### Next Steps:\n",
    "- Review the generated figure in the `results/` directory\n",
    "- Use the SVG version for publication-quality figures\n",
    "- Analyze the performance patterns across complexity bins\n",
    "\n",
    "---\n",
    "*This analysis is part of the ELDER project. For more information, see the README.md file.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}